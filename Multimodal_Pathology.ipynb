{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4op97VjxzpUH"
      },
      "source": [
        "process files in a subfolder (needs new path for each case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cle24nXAzpUJ"
      },
      "outputs": [],
      "source": [
        "import openslide\n",
        "from openslide import open_slide\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import openslide\n",
        "from openslide import open_slide\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from openslide import open_slide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tifffile as tiff\n",
        "from openslide.deepzoom import DeepZoomGenerator\n",
        "import os\n",
        "def norm_HnE(img, Io=240, alpha=1, beta=0.15):\n",
        "\n",
        "\n",
        "    ######## Step 1: Convert RGB to OD ###################\n",
        "    ## reference H&E OD matrix.\n",
        "    #Can be updated if you know the best values for your image. \n",
        "    #Otherwise use the following default values. \n",
        "    #Read the above referenced papers on this topic. \n",
        "    HERef = np.array([[0.5626, 0.2159],\n",
        "                      [0.7201, 0.8012],\n",
        "                      [0.4062, 0.5581]])\n",
        "    ### reference maximum stain concentrations for H&E\n",
        "    maxCRef = np.array([1.9705, 1.0308])\n",
        "    \n",
        "    \n",
        "    # extract the height, width and num of channels of image\n",
        "    h, w, c = img.shape\n",
        "    \n",
        "    # reshape image to multiple rows and 3 columns.\n",
        "    #Num of rows depends on the image size (wxh)\n",
        "    img = img.reshape((-1,3))\n",
        "    \n",
        "    # calculate optical density\n",
        "    # OD = −log10(I)  \n",
        "    #OD = -np.log10(img+0.004)  #Use this when reading images with skimage\n",
        "    #Adding 0.004 just to avoid log of zero. \n",
        "    \n",
        "    OD = -np.log10((img.astype(np.float)+1)/Io) #Use this for opencv imread\n",
        "    #Add 1 in case any pixels in the image have a value of 0 (log 0 is indeterminate)\n",
        "    \n",
        "    \n",
        "    ############ Step 2: Remove data with OD intensity less than β ############\n",
        "    # remove transparent pixels (clear region with no tissue)\n",
        "    ODhat = OD[~np.any(OD < beta, axis=1)] #Returns an array where OD values are above beta\n",
        "    #Check by printing ODhat.min()\n",
        "    \n",
        "    ############# Step 3: Calculate SVD on the OD tuples ######################\n",
        "    #Estimate covariance matrix of ODhat (transposed)\n",
        "    # and then compute eigen values & eigenvectors.\n",
        "    eigvals, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n",
        "    \n",
        "    \n",
        "    ######## Step 4: Create plane from the SVD directions with two largest values ######\n",
        "    #project on the plane spanned by the eigenvectors corresponding to the two \n",
        "    # largest eigenvalues    \n",
        "    That = ODhat.dot(eigvecs[:,1:3]) #Dot product\n",
        "    \n",
        "    ############### Step 5: Project data onto the plane, and normalize to unit length ###########\n",
        "    ############## Step 6: Calculate angle of each point wrt the first SVD direction ########\n",
        "    #find the min and max vectors and project back to OD space\n",
        "    phi = np.arctan2(That[:,1],That[:,0])\n",
        "    \n",
        "    minPhi = np.percentile(phi, alpha)\n",
        "    maxPhi = np.percentile(phi, 100-alpha)\n",
        "    \n",
        "    vMin = eigvecs[:,1:3].dot(np.array([(np.cos(minPhi), np.sin(minPhi))]).T)\n",
        "    vMax = eigvecs[:,1:3].dot(np.array([(np.cos(maxPhi), np.sin(maxPhi))]).T)\n",
        "    \n",
        "    \n",
        "    # a heuristic to make the vector corresponding to hematoxylin first and the \n",
        "    # one corresponding to eosin second\n",
        "    if vMin[0] > vMax[0]:    \n",
        "        HE = np.array((vMin[:,0], vMax[:,0])).T\n",
        "        \n",
        "    else:\n",
        "        HE = np.array((vMax[:,0], vMin[:,0])).T\n",
        "    \n",
        "    \n",
        "    # rows correspond to channels (RGB), columns to OD values\n",
        "    Y = np.reshape(OD, (-1, 3)).T\n",
        "    \n",
        "    # determine concentrations of the individual stains\n",
        "    C = np.linalg.lstsq(HE,Y, rcond=None)[0]\n",
        "    \n",
        "    # normalize stain concentrations\n",
        "    maxC = np.array([np.percentile(C[0,:], 99), np.percentile(C[1,:],99)])\n",
        "    tmp = np.divide(maxC,maxCRef)\n",
        "    C2 = np.divide(C,tmp[:, np.newaxis])\n",
        "    \n",
        "    ###### Step 8: Convert extreme values back to OD space\n",
        "    # recreate the normalized image using reference mixing matrix \n",
        "    \n",
        "    Inorm = np.multiply(Io, np.exp(-HERef.dot(C2)))\n",
        "    Inorm[Inorm>255] = 254\n",
        "    Inorm = np.reshape(Inorm.T, (h, w, 3)).astype(np.uint8)  \n",
        "    \n",
        "    # Separating H and E components\n",
        "    \n",
        "    H = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,0], axis=1).dot(np.expand_dims(C2[0,:], axis=0))))\n",
        "    H[H>255] = 254\n",
        "    H = np.reshape(H.T, (h, w, 3)).astype(np.uint8)\n",
        "    \n",
        "    E = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,1], axis=1).dot(np.expand_dims(C2[1,:], axis=0))))\n",
        "    E[E>255] = 254\n",
        "    E = np.reshape(E.T, (h, w, 3)).astype(np.uint8)\n",
        "    \n",
        "    return (Inorm, H, E)\n",
        "    \n",
        "for root, dirs, files in os.walk(os.path.abspath(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\TCGA-4P-AA8J\")):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n",
        "        pathi = os.path.join(root, file)\n",
        "        slide = open_slide(pathi)\n",
        "        pathi2= os.path.dirname(pathi)\n",
        "        #slide = open_slide(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\TCGA-4P-AA8J\\TCGA-4P-AA8J-01A-01-TS1.D62B960E-A357-4E3D-9CB5-93F112174546.svs\")\n",
        "\n",
        "        #Generate object for tiles using the DeepZoomGenerator\n",
        "        tiles = DeepZoomGenerator(slide, tile_size=1000, overlap=0, limit_bounds=False)\n",
        "        #Here, we have divided our svs into tiles of size 256 with no overlap. \n",
        "\n",
        "        #The tiles object also contains data at many levels. \n",
        "        #To check the number of levels\n",
        "        print(\"The number of levels in the tiles object are: \", tiles.level_count)\n",
        "        print(\"The dimensions of data in each level are: \", tiles.level_dimensions)\n",
        "        #Total number of tiles in the tiles object\n",
        "        print(\"Total number of tiles = : \", tiles.tile_count)\n",
        "\n",
        "        level_count_number = tiles.level_count - 1\n",
        "        ###### processing and saving each tile to local directory\n",
        "        cols, rows = tiles.level_tiles[level_count_number]\n",
        "\n",
        "\n",
        "        #orig_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles\"\n",
        "        norm_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"\n",
        "        #H_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles\"\n",
        "        #E_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles\"\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(orig_tile_dir_name) == False:\n",
        "                #os.makedirs(orig_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        if os.path.isdir(norm_tile_dir_name) == False:\n",
        "                os.makedirs(norm_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(H_tile_dir_name) == False:\n",
        "                #os.makedirs(H_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(E_tile_dir_name) == False:\n",
        "                #os.makedirs(E_tile_dir_name)\n",
        "\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                tile_name = str(col) + \"_\" + str(row)\n",
        "                #tile_name = os.path.join(tile_dir, '%d_%d' % (col, row))\n",
        "                #print(\"Now processing tile with title: \", tile_name)\n",
        "                temp_tile = tiles.get_tile(level_count_number, (col, row))\n",
        "                temp_tile_RGB = temp_tile.convert('RGB')\n",
        "                temp_tile_np = np.array(temp_tile_RGB)\n",
        "                #Save original tile\n",
        "                #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles/\"+tile_name + \"_original.tif\", temp_tile_np)\n",
        "        \n",
        "                if temp_tile_np.mean() < 200:\n",
        "                    print(\"Processing tile number:\", tile_name)\n",
        "                    norm_img, H_img, E_img = norm_HnE(temp_tile_np, Io=240, alpha=1, beta=0.15)\n",
        "                #Save the norm tile, H and E tiles      \n",
        "            \n",
        "                    tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles/\"+tile_name + \"_norm.tif\", norm_img)\n",
        "                    #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles/\"+tile_name + \"_H.tif\", H_img)\n",
        "                    #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles/\" +tile_name + \"_E.tif\", E_img)\n",
        "            \n",
        "                else:\n",
        "                    print(\"NOT PROCESSING TILE:\", tile_name)\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "path = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles/\"\n",
        "# Delete images with the high pixel value\n",
        "for filename in os.listdir(path):\n",
        "    images = Image.open(os.path.join(path,filename))\n",
        "    width, height = images.size\n",
        "    print(images.size)\n",
        "    if images.size != (1000, 1000):\n",
        "        images.close()\n",
        "        os.remove(os.path.join(path, filename))\n",
        "images.close()\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "files_list = []\n",
        "\n",
        "for root, dirs, files in os.walk(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"):\n",
        "    for file in files:\n",
        "        #all \n",
        "        if file.endswith(\".tif\"):\n",
        "            files_list.append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "#print images\n",
        "#lets me count and print the amount of jpeg,jpg,pmg \n",
        "file_count = len(files_list)\n",
        "print (file_count)\n",
        "\n",
        "# print files_list   \n",
        "filesToCopy = random.sample(files_list, 10)  #prints two random files from list \n",
        "\n",
        "destPath = pathi2\n",
        "\n",
        "# if destination dir does not exists, create it\n",
        "if os.path.isdir(destPath) == False:\n",
        "        os.makedirs(destPath)\n",
        "\n",
        "# iteraate over all random files and move them\n",
        "for file in filesToCopy:\n",
        "    shutil.move(file, destPath)\n",
        "\n",
        "source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"\n",
        "destination = pathi2\n",
        "dest = shutil.move(source, destination) \n",
        "\n",
        "#source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles\"\n",
        "#destination = pathi2\n",
        "#dest = shutil.move(source, destination) \n",
        "\n",
        "#source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles\"\n",
        "#destination = pathi2\n",
        "#dest = shutil.move(source, destination) \n",
        "\n",
        "#source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles\"\n",
        "#destination = pathi2\n",
        "#dest = shutil.move(source, destination) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R67bj8ZzpUM"
      },
      "source": [
        "Process all subfolders and files within it in a root folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoK2kQcSzpUM"
      },
      "outputs": [],
      "source": [
        "import openslide\n",
        "from openslide import open_slide\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import openslide\n",
        "from openslide import open_slide\n",
        "import openslide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from openslide import open_slide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tifffile as tiff\n",
        "from openslide.deepzoom import DeepZoomGenerator\n",
        "import os\n",
        "def norm_HnE(img, Io=240, alpha=1, beta=0.15):\n",
        "\n",
        "\n",
        "    ######## Step 1: Convert RGB to OD ###################\n",
        "    ## reference H&E OD matrix.\n",
        "    #Can be updated if you know the best values for your image. \n",
        "    #Otherwise use the following default values. \n",
        "    #Read the above referenced papers on this topic. \n",
        "    HERef = np.array([[0.5626, 0.2159],\n",
        "                      [0.7201, 0.8012],\n",
        "                      [0.4062, 0.5581]])\n",
        "    ### reference maximum stain concentrations for H&E\n",
        "    maxCRef = np.array([1.9705, 1.0308])\n",
        "    \n",
        "    \n",
        "    # extract the height, width and num of channels of image\n",
        "    h, w, c = img.shape\n",
        "    \n",
        "    # reshape image to multiple rows and 3 columns.\n",
        "    #Num of rows depends on the image size (wxh)\n",
        "    img = img.reshape((-1,3))\n",
        "    \n",
        "    # calculate optical density\n",
        "    # OD = −log10(I)  \n",
        "    #OD = -np.log10(img+0.004)  #Use this when reading images with skimage\n",
        "    #Adding 0.004 just to avoid log of zero. \n",
        "    \n",
        "    OD = -np.log10((img.astype(np.float)+1)/Io) #Use this for opencv imread\n",
        "    #Add 1 in case any pixels in the image have a value of 0 (log 0 is indeterminate)\n",
        "    \n",
        "    \n",
        "    ############ Step 2: Remove data with OD intensity less than β ############\n",
        "    # remove transparent pixels (clear region with no tissue)\n",
        "    ODhat = OD[~np.any(OD < beta, axis=1)] #Returns an array where OD values are above beta\n",
        "    #Check by printing ODhat.min()\n",
        "    \n",
        "    ############# Step 3: Calculate SVD on the OD tuples ######################\n",
        "    #Estimate covariance matrix of ODhat (transposed)\n",
        "    # and then compute eigen values & eigenvectors.\n",
        "    eigvals, eigvecs = np.linalg.eigh(np.cov(ODhat.T))\n",
        "    \n",
        "    \n",
        "    ######## Step 4: Create plane from the SVD directions with two largest values ######\n",
        "    #project on the plane spanned by the eigenvectors corresponding to the two \n",
        "    # largest eigenvalues    \n",
        "    That = ODhat.dot(eigvecs[:,1:3]) #Dot product\n",
        "    \n",
        "    ############### Step 5: Project data onto the plane, and normalize to unit length ###########\n",
        "    ############## Step 6: Calculate angle of each point wrt the first SVD direction ########\n",
        "    #find the min and max vectors and project back to OD space\n",
        "    phi = np.arctan2(That[:,1],That[:,0])\n",
        "    \n",
        "    minPhi = np.percentile(phi, alpha)\n",
        "    maxPhi = np.percentile(phi, 100-alpha)\n",
        "    \n",
        "    vMin = eigvecs[:,1:3].dot(np.array([(np.cos(minPhi), np.sin(minPhi))]).T)\n",
        "    vMax = eigvecs[:,1:3].dot(np.array([(np.cos(maxPhi), np.sin(maxPhi))]).T)\n",
        "    \n",
        "    \n",
        "    # a heuristic to make the vector corresponding to hematoxylin first and the \n",
        "    # one corresponding to eosin second\n",
        "    if vMin[0] > vMax[0]:    \n",
        "        HE = np.array((vMin[:,0], vMax[:,0])).T\n",
        "        \n",
        "    else:\n",
        "        HE = np.array((vMax[:,0], vMin[:,0])).T\n",
        "    \n",
        "    \n",
        "    # rows correspond to channels (RGB), columns to OD values\n",
        "    Y = np.reshape(OD, (-1, 3)).T\n",
        "    \n",
        "    # determine concentrations of the individual stains\n",
        "    C = np.linalg.lstsq(HE,Y, rcond=None)[0]\n",
        "    \n",
        "    # normalize stain concentrations\n",
        "    maxC = np.array([np.percentile(C[0,:], 99), np.percentile(C[1,:],99)])\n",
        "    tmp = np.divide(maxC,maxCRef)\n",
        "    C2 = np.divide(C,tmp[:, np.newaxis])\n",
        "    \n",
        "    ###### Step 8: Convert extreme values back to OD space\n",
        "    # recreate the normalized image using reference mixing matrix \n",
        "    \n",
        "    Inorm = np.multiply(Io, np.exp(-HERef.dot(C2)))\n",
        "    Inorm[Inorm>255] = 254\n",
        "    Inorm = np.reshape(Inorm.T, (h, w, 3)).astype(np.uint8)  \n",
        "    \n",
        "    # Separating H and E components\n",
        "    \n",
        "    H = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,0], axis=1).dot(np.expand_dims(C2[0,:], axis=0))))\n",
        "    H[H>255] = 254\n",
        "    H = np.reshape(H.T, (h, w, 3)).astype(np.uint8)\n",
        "    \n",
        "    E = np.multiply(Io, np.exp(np.expand_dims(-HERef[:,1], axis=1).dot(np.expand_dims(C2[1,:], axis=0))))\n",
        "    E[E>255] = 254\n",
        "    E = np.reshape(E.T, (h, w, 3)).astype(np.uint8)\n",
        "    \n",
        "    return (Inorm, H, E)\n",
        "\n",
        "\n",
        "tumb = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\test\"\n",
        "\n",
        "# for root, dirs, files in os.walk(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\test\"):\n",
        "for dirs in os.listdir(tumb):\n",
        "    # if len(files) == 0:\n",
        "    #     continue\n",
        "    # else:\n",
        "    for file in os.listdir(os.path.join(tumb, dirs)):\n",
        "        pathi = os.path.join(tumb,dirs,file)\n",
        "\n",
        "        # print(os.path.join(root, file))\n",
        "        # pathi = os.path.join(root, file)\n",
        "        slide = open_slide(pathi)\n",
        "        pathi2= os.path.dirname(pathi)\n",
        "        #slide = open_slide(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\TCGA-4P-AA8J\\TCGA-4P-AA8J-01A-01-TS1.D62B960E-A357-4E3D-9CB5-93F112174546.svs\")\n",
        "\n",
        "        #Generate object for tiles using the DeepZoomGenerator\n",
        "        tiles = DeepZoomGenerator(slide, tile_size=1000, overlap=0, limit_bounds=False)\n",
        "        #Here, we have divided our svs into tiles of size 256 with no overlap. \n",
        "\n",
        "        #The tiles object also contains data at many levels. \n",
        "        #To check the number of levels\n",
        "        print(\"The number of levels in the tiles object are: \", tiles.level_count)\n",
        "        print(\"The dimensions of data in each level are: \", tiles.level_dimensions)\n",
        "        #Total number of tiles in the tiles object\n",
        "        print(\"Total number of tiles = : \", tiles.tile_count)\n",
        "\n",
        "        level_count_number = tiles.level_count - 1\n",
        "        ###### processing and saving each tile to local directory\n",
        "        cols, rows = tiles.level_tiles[level_count_number] \n",
        "\n",
        "\n",
        "        #orig_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles\"\n",
        "        norm_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"\n",
        "        #H_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles\"\n",
        "        #E_tile_dir_name = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles\"\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(orig_tile_dir_name) == False:\n",
        "                #os.makedirs(orig_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        if os.path.isdir(norm_tile_dir_name) == False:\n",
        "                os.makedirs(norm_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(H_tile_dir_name) == False:\n",
        "                #os.makedirs(H_tile_dir_name)\n",
        "\n",
        "        # if destination dir does not exists, create it\n",
        "        #if os.path.isdir(E_tile_dir_name) == False:\n",
        "                #os.makedirs(E_tile_dir_name)\n",
        "\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                tile_name = str(col) + \"_\" + str(row)\n",
        "                #tile_name = os.path.join(tile_dir, '%d_%d' % (col, row))\n",
        "                #print(\"Now processing tile with title: \", tile_name)\n",
        "                temp_tile = tiles.get_tile(level_count_number, (col, row))\n",
        "                temp_tile_RGB = temp_tile.convert('RGB')\n",
        "                temp_tile_np = np.array(temp_tile_RGB)\n",
        "                #Save original tile\n",
        "                #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles/\"+tile_name + \"_original.tif\", temp_tile_np)\n",
        "        \n",
        "                if temp_tile_np.mean() < 200:\n",
        "                    print(\"Processing tile number:\", tile_name)\n",
        "                    norm_img, H_img, E_img = norm_HnE(temp_tile_np, Io=240, alpha=1, beta=0.15)\n",
        "                #Save the norm tile, H and E tiles      \n",
        "            \n",
        "                    tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles/\"+tile_name + \"_norm.tif\", norm_img)\n",
        "                    #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles/\"+tile_name + \"_H.tif\", H_img)\n",
        "                    #tiff.imsave(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles/\" +tile_name + \"_E.tif\", E_img)\n",
        "            \n",
        "                else:\n",
        "                    print(\"NOT PROCESSING TILE:\", tile_name)\n",
        "\n",
        "\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import os\n",
        "\n",
        "    path = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles/\"\n",
        "    # Delete images with the high pixel value\n",
        "    for filename in os.listdir(path):\n",
        "        images = Image.open(os.path.join(path,filename))\n",
        "        width, height = images.size\n",
        "        print(images.size)\n",
        "        if images.size != (1000, 1000):\n",
        "            images.close()\n",
        "            os.remove(os.path.join(path, filename))\n",
        "    images.close()\n",
        "\n",
        "    import os\n",
        "    import random\n",
        "    import shutil\n",
        "\n",
        "\n",
        "    files_list = []\n",
        "\n",
        "    for root, dirs, files in os.walk(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"):\n",
        "        for file in files:\n",
        "            #all \n",
        "            if file.endswith(\".tif\"):\n",
        "                files_list.append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "    #print images\n",
        "    #lets me count and print the amount of jpeg,jpg,pmg \n",
        "    file_count = len(files_list)\n",
        "    print (file_count)\n",
        "\n",
        "    # print files_list   \n",
        "    filesToCopy = random.sample(files_list, 10)  #prints two random files from list \n",
        "\n",
        "    destPath = pathi2\n",
        "\n",
        "    # if destination dir does not exists, create it\n",
        "    if os.path.isdir(destPath) == False:\n",
        "            os.makedirs(destPath)\n",
        "\n",
        "    # iteraate over all random files and move them\n",
        "    for file in filesToCopy:\n",
        "        shutil.move(file, destPath)\n",
        "\n",
        "    source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\normalized_tiles\"\n",
        "    destination = pathi2\n",
        "    dest = shutil.move(source, destination) \n",
        "\n",
        "    #source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\H_tiles\"\n",
        "    #destination = pathi2\n",
        "    #dest = shutil.move(source, destination) \n",
        "\n",
        "    #source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\E_tiles\"\n",
        "    #destination = pathi2\n",
        "    #dest = shutil.move(source, destination) \n",
        "\n",
        "    #source = r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\original_tiles\"\n",
        "    #destination = pathi2\n",
        "    #dest = shutil.move(source, destination) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-m-0xDmzpUO"
      },
      "outputs": [],
      "source": [
        "#import pyvips\n",
        "from openslide import open_slide\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tifffile as tiff\n",
        "\n",
        "#Extract a small region from the large file (level 0)\n",
        "#Let us extract a region from somewhere in the middle - coords 16k, 16k\n",
        "#Extract 1024,1024 region\n",
        "smaller_region = slide.read_region((4000,4000), 0, (1024,1024))\n",
        "smaller_region_RGB = smaller_region.convert('RGB')\n",
        "smaller_region_np = np.array(smaller_region_RGB)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(smaller_region_np)\n",
        "\n",
        "norm_img, H_img, E_img = norm_HnE(smaller_region_np, Io=240, alpha=1, beta=0.15)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(221)\n",
        "plt.title('Original Image')\n",
        "plt.imshow(smaller_region_np)\n",
        "plt.subplot(222)\n",
        "plt.title('Normalized Image')\n",
        "plt.imshow(norm_img)\n",
        "plt.subplot(223)\n",
        "plt.title('H image')\n",
        "plt.imshow(H_img)\n",
        "plt.subplot(224)\n",
        "plt.title('E image')\n",
        "plt.imshow(E_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ol3L6RuzpUO"
      },
      "outputs": [],
      "source": [
        "from openslide.deepzoom import DeepZoomGenerator\n",
        "#slide = open_slide(r\"C:\\Users\\Babak\\Desktop\\Pathogenomics\\TCGA_data_OSCC_cases\\TCGA-4P-AA8J\\TCGA-4P-AA8J-01A-01-TS1.D62B960E-A357-4E3D-9CB5-93F112174546.svs\")\n",
        "\n",
        "#Generate object for tiles using the DeepZoomGenerator\n",
        "tiles = DeepZoomGenerator(slide, tile_size=1000, overlap=0, limit_bounds=False)\n",
        "#Here, we have divided our svs into tiles of size 256 with no overlap. \n",
        "\n",
        "#The tiles object also contains data at many levels. \n",
        "#To check the number of levels\n",
        "print(\"The number of levels in the tiles object are: \", tiles.level_count)\n",
        "print(\"The dimensions of data in each level are: \", tiles.level_dimensions)\n",
        "#Total number of tiles in the tiles object\n",
        "print(\"Total number of tiles = : \", tiles.tile_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wd9ViLzzpUO"
      },
      "source": [
        "Excel_Merge_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSo8GHoIzpUO"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        " \n",
        "# specifying the path to csv files\n",
        "path = r\"C:\\Users\\Babak\\Downloads\\bftools\\bftools\\splitted\"\n",
        " \n",
        "# csv files in the path\n",
        "file_list = glob.glob(path + \"/*.csv\")\n",
        " \n",
        "# list of excel files we want to merge.\n",
        "# pd.read_excel(file_path) reads the \n",
        "# excel data into pandas dataframe.\n",
        "excl_list = []\n",
        " \n",
        "for file in file_list:\n",
        "    tato = pd.read_csv(file)\n",
        "    tato.loc['mean'] = tato.mean()\n",
        "    tato = tato.drop(tato.index.to_list()[0:-1] ,axis = 0 )\n",
        "    excl_list.append(tato)\n",
        "\n",
        " \n",
        "# concatenate all DataFrames in the list\n",
        "# into a single DataFrame, returns new\n",
        "# DataFrame.\n",
        "\n",
        "excl_merged = pd.concat(excl_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm5GYP8_zpUO"
      },
      "outputs": [],
      "source": [
        "print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDvT5NeyzpUP"
      },
      "outputs": [],
      "source": [
        "excl_merged.to_csv(r\"C:\\Users\\Babak\\Downloads\\bftools\\bftools\\splitted\\Bank_Stocks.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aXq2-CPzpUP"
      },
      "outputs": [],
      "source": [
        "excl_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws63CsqnzpUP"
      },
      "outputs": [],
      "source": [
        "excl_merged.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0NsFKlazpUP"
      },
      "outputs": [],
      "source": [
        "excl_merged.loc['mean'] = excl_merged.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMoeYqJNzpUP"
      },
      "outputs": [],
      "source": [
        "excl_merged = excl_merged.drop(excl_merged.index.to_list()[0:-1] ,axis = 0 )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "30231eece09b4e58137f7de92a1edff1ce67d1eb6dc3e77c83046d0c2c0004f0"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}